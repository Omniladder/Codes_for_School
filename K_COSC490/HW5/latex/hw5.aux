\relax 
\providecommand\zref@newlabel[2]{}
\providecommand{\transparent@use}[1]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Concepts, intuitions and big picture}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Self-Attention and Transformers}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Complexity}{4}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Masking in Self-Attention}{4}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Programming}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Skeleton Code and Structure:}{5}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Submission:}{5}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}A GPT Language Model from Scratch}{5}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Introduction: Self-attention and Transformer}{5}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Some useful documentation:}{5}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}What is Attention?}{6}{subsubsection.3.1.2}\protected@file@percent }
\citation{vaswani2017attention}
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Scaled Dot Product Attention}{7}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Multi-Head Attention}{7}{subsubsection.3.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.5}Causal Attention Masking}{8}{subsubsection.3.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.6}Putting it Together (so far): Multi-Head Self Attention with Causal Masking}{8}{subsubsection.3.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.7}Residual connection and normalization}{8}{subsubsection.3.1.7}\protected@file@percent }
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.8}Putting it Together: Transformer Architecture}{9}{subsubsection.3.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.9}Train and Evaluate GPT LM on GPUs}{9}{subsubsection.3.1.9}\protected@file@percent }
\newlabel{subsubsec:gpu}{{3.1.9}{9}{Train and Evaluate GPT LM on GPUs}{subsubsection.3.1.9}{}}
\bibstyle{apalike}
\bibdata{ref}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces loss and ppl of the GPT LM}}{11}{figure.caption.11}\protected@file@percent }
\gdef \@abspage@last{11}
